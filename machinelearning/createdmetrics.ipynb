{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "\n",
    "Accuracy measures how often the model is correct.\n",
    "\n",
    "How to Calculate\n",
    "(True Positive + True Negative) / Total Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.822\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "actual = numpy.random.binomial(1,.9,size = 1000)\n",
    "predicted = numpy.random.binomial(1,.9,size = 1000)\n",
    "\n",
    "Accuracy = metrics.accuracy_score(actual, predicted)\n",
    "\n",
    "print(Accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision\n",
    "\n",
    "Of the positives predicted, what percentage is truly positive?\n",
    "\n",
    "How to Calculate\n",
    "True Positive / (True Positive + False Positive)\n",
    "\n",
    "Precision does not evaluate the correctly predicted negative cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9014396456256921\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "actual = numpy.random.binomial(1,.9,size = 1000)\n",
    "predicted = numpy.random.binomial(1,.9,size = 1000)\n",
    "\n",
    "Precision = metrics.precision_score(actual, predicted)\n",
    "\n",
    "print(Precision)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity (Recall)\n",
    "\n",
    "Of all the positive cases, what percentage are predicted positive?\n",
    "\n",
    "Sensitivity (sometimes called Recall) measures how good the model is at predicting positives.\n",
    "\n",
    "This means it looks at true positives and false negatives (which are positives that have been incorrectly predicted as negative).\n",
    "\n",
    "How to Calculate\n",
    "True Positive / (True Positive + False Negative)\n",
    "\n",
    "Sensitivity is good at understanding how well the model predicts something is positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9154616240266963\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "actual = numpy.random.binomial(1,.9,size = 1000)\n",
    "predicted = numpy.random.binomial(1,.9,size = 1000)\n",
    "\n",
    "Sensitivity_recall = metrics.recall_score(actual, predicted)\n",
    "\n",
    "print(Sensitivity_recall)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specificity\n",
    "\n",
    "How well the model is at prediciting negative results?\n",
    "\n",
    "Specificity is similar to sensitivity, but looks at it from the persepctive of negative results.\n",
    "\n",
    "How to Calculate\n",
    "True Negative / (True Negative + False Positive)\n",
    "\n",
    "Since it is just the opposite of Recall, we use the recall_score function, taking the opposite position label:import numpy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07207207207207207\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "actual = numpy.random.binomial(1,.9,size = 1000)\n",
    "predicted = numpy.random.binomial(1,.9,size = 1000)\n",
    "\n",
    "Specificity = metrics.recall_score(actual, predicted, pos_label=0)\n",
    "\n",
    "print(Specificity)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F-score\n",
    "\n",
    "F-score is the \"harmonic mean\" of precision and sensitivity.\n",
    "\n",
    "It considers both false positive and false negative cases and is good for imbalanced datasets.\n",
    "\n",
    "How to Calculate\n",
    "2 * ((Precision * Sensitivity) / (Precision + Sensitivity))\n",
    "\n",
    "This score does not take into consideration the True Negative values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9115870400878638\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "actual = numpy.random.binomial(1,.9,size = 1000)\n",
    "predicted = numpy.random.binomial(1,.9,size = 1000)\n",
    "\n",
    "F1_score = metrics.f1_score(actual, predicted)\n",
    "\n",
    "print(F1_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All calulations in one:\n",
    "\n",
    "Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.831, 'Precision': 0.8980263157894737, 'Sensitivity_recall': 0.9150837988826815, 'Specificity': 0.11428571428571428, 'F1_score': 0.9064748201438848}\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn import metrics\n",
    "\n",
    "actual = numpy.random.binomial(1,.9,size = 1000)\n",
    "predicted = numpy.random.binomial(1,.9,size = 1000)\n",
    "\n",
    "Accuracy = metrics.accuracy_score(actual, predicted)\n",
    "Precision = metrics.precision_score(actual, predicted)\n",
    "Sensitivity_recall = metrics.recall_score(actual, predicted)\n",
    "Specificity = metrics.recall_score(actual, predicted, pos_label=0)\n",
    "F1_score = metrics.f1_score(actual, predicted)\n",
    "\n",
    "#metrics:\n",
    "print({\"Accuracy\":Accuracy,\"Precision\":Precision,\"Sensitivity_recall\":Sensitivity_recall,\"Specificity\":Specificity,\"F1_score\":F1_score})\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
